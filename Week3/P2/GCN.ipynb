{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mlOkawZgDv"
      },
      "source": [
        "Build and train a graph convolutional neural network using PyTorch Geometric for the node property prediction task.\n",
        "\n",
        "We will use ogbn-products dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Vh9kosaChl"
      },
      "source": [
        "## OGBN-Products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HFHJC-LaLyi"
      },
      "source": [
        "The ogbn-products dataset is an undirected and unweighted graph, representing an Amazon product co-purchasing network. Nodes represent products sold in Amazon, and edges between two products indicate that the products are purchased together. Node features are generated by extracting bag-of-words features from the product descriptions followed by a Principal Component Analysis to reduce the dimension to 100.\n",
        "\n",
        "The task is to predict the category of a product in a multi-class classification setup, where the 47 top-level categories are used for target labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vkP8pA1qBE5",
        "outputId": "87773851-6b45-4f2d-bf97-3b082a118941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch has version 2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6d22O6DqGSZ"
      },
      "source": [
        "Download the necessary packages for PyG. Make sure that your version of torch matches the output from the cell above. In case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr8hfxJ-qRg2",
        "outputId": "c14bab71-c29d-4f9c-b711-e370c15b7cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt20cu118\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt20cu118\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910460 sha256=e5388d85cfb824bc3b6dc400be27c9977a8f7c874da5dd25398c2e13a0852807\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.16)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=27ba1806e2222ee4733f76f137fa07a11fc00a4fc1c0b581f627978dc4dce271\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "# Install torch geometric\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric\n",
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3SkS1Mzcbe8k"
      },
      "outputs": [],
      "source": [
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import DataLoader\n",
        "import numpy as np\n",
        "from torch_geometric.typing import SparseTensor\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ibJ0ieoIwQM",
        "outputId": "484074ce-2007-4a03-8ccc-7946aea5b06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloaded 1.38 GB: 100%|██████████| 1414/1414 [00:26<00:00, 53.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset/products.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 87.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "dataset_name = 'ogbn-products'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                 transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# If you use GPU, the device should be cuda\n",
        "print('Device: {}'.format(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ6I9prjdg57",
        "outputId": "ba9c9605-dd5e-428c-d7e7-fef66da8500d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(num_nodes=2449029, x=[2449029, 100], y=[2449029, 1], adj_t=[2449029, 2449029, nnz=123718280])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7QzFWvS2DvC"
      },
      "source": [
        "This dataset is very big and if you try to run it as it is on colab, you may get an out of memory error.\n",
        "\n",
        "One solution is to use batching and train on subgraphs. Here, we will just make a smaller dataset so that we can train it in one go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q3Nrhnno6ylE"
      },
      "outputs": [],
      "source": [
        "# We need to have edge indxes to make a subgraph. We can get those from the adjacency matrix.\n",
        "data.edge_index = torch.stack([data.adj_t.__dict__[\"storage\"]._row, data.adj_t.__dict__[\"storage\"]._col])\n",
        "\n",
        "# We will only use the first 100000 nodes.\n",
        "sub_nodes = 100000\n",
        "sub_graph = data.subgraph(torch.arange(sub_nodes))\n",
        "\n",
        "# Update the adjaceny matrix according to the new graph\n",
        "sub_graph.adj_t = SparseTensor(\n",
        "    row=sub_graph.edge_index[0],\n",
        "    col=sub_graph.edge_index[1],\n",
        "    sparse_sizes=None,\n",
        "    is_sorted=True,\n",
        "    trust_data=True,\n",
        ")\n",
        "\n",
        "sub_graph = sub_graph.to(device)\n",
        "\n",
        "data = sub_graph\n",
        "data = data.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJr8y1mG7xN0",
        "outputId": "c97d81d5-3daa-4d46-d9d1-406e68046e9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train': tensor([88473, 12137, 66526,  ..., 19282,  2687, 92877]),\n",
              " 'valid': tensor([87559, 71191,  1913,  ..., 85485, 80590, 14361]),\n",
              " 'test': tensor([49367, 69282, 89334,  ..., 55099, 66480, 47161])}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Spilt data into train validation and test set\n",
        "split_sizes = [int(sub_nodes*0.8),int(sub_nodes*0.05),int(sub_nodes*0.15)]\n",
        "indices = torch.arange(sub_nodes)\n",
        "np.random.shuffle(indices.numpy())\n",
        "split_idx = {s:t for t,s in zip(torch.split(indices, split_sizes, dim=0), [\"train\", \"valid\", \"test\"])}\n",
        "split_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qiB5rR-02LP_"
      },
      "outputs": [],
      "source": [
        "train_idx = split_idx['train'].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfHKG2kTcEOe",
        "outputId": "69717df9-03d9-4011-b98e-e543835e956e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Length of each node: 100\n"
          ]
        }
      ],
      "source": [
        "print(f\"Feature Length of each node: {data.x.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "## GCN Model\n",
        "\n",
        "Now we will implement our GCN model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "m1RLX9bQ1Q9u"
      },
      "outputs": [],
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, return_embeds=False):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gcnlayers = nn.ModuleList([\n",
        "            GCNConv(input_dim, hidden_dim),\n",
        "            GCNConv(hidden_dim, hidden_dim),\n",
        "            GCNConv(hidden_dim, hidden_dim),\n",
        "            GCNConv(hidden_dim, hidden_dim)\n",
        "        ])\n",
        "        self.last_gcn = GCNConv(hidden_dim, output_dim)\n",
        "        self.bns = nn.ModuleList([\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim)\n",
        "        ])\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.gcnlayers:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "        for i in range(len(self.gcnlayers)):\n",
        "            x = self.gcnlayers[i](x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.last_gcn(x, adj_t)\n",
        "\n",
        "        if not self.return_embeds:\n",
        "          x = self.softmax(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lDSFAtXU16o-"
      },
      "outputs": [],
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t)\n",
        "    loss = loss_fn(out[train_idx], data.y[train_idx].reshape(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QKOum46J18XN"
      },
      "outputs": [],
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(data.x, data.adj_t)\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    if save_model_results:\n",
        "      print (\"Saving Model Predictions\")\n",
        "\n",
        "      data = {}\n",
        "      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      df.to_csv('ogbn-products_node.csv', sep=',', index=False)\n",
        "\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "E8OVl-gq1-YF"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'hidden_dim': 256,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 200,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "T4NoqHjH1_7i"
      },
      "outputs": [],
      "source": [
        "model = GCN(data.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes).to(device)\n",
        "evaluator = Evaluator(name='ogbn-products')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCyy_-xb2Ctn",
        "outputId": "314c1b77-544b-4d98-83f2-80ab71b55235"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-7c3925edf625>:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01, Loss: 4.2937, Train: 24.46%, Valid: 25.30% Test: 23.94%\n",
            "Epoch: 02, Loss: 2.2312, Train: 67.28%, Valid: 66.30% Test: 67.13%\n",
            "Epoch: 03, Loss: 1.2245, Train: 71.13%, Valid: 69.72% Test: 71.21%\n",
            "Epoch: 04, Loss: 0.9818, Train: 79.23%, Valid: 78.52% Test: 79.18%\n",
            "Epoch: 05, Loss: 0.8637, Train: 81.38%, Valid: 80.50% Test: 81.49%\n",
            "Epoch: 06, Loss: 0.7992, Train: 82.27%, Valid: 81.16% Test: 82.23%\n",
            "Epoch: 07, Loss: 0.7397, Train: 83.43%, Valid: 82.76% Test: 83.51%\n",
            "Epoch: 08, Loss: 0.6968, Train: 84.69%, Valid: 84.26% Test: 84.63%\n",
            "Epoch: 09, Loss: 0.6703, Train: 85.44%, Valid: 84.80% Test: 85.20%\n",
            "Epoch: 10, Loss: 0.6473, Train: 85.79%, Valid: 84.90% Test: 85.53%\n",
            "Epoch: 11, Loss: 0.6284, Train: 85.79%, Valid: 85.00% Test: 85.62%\n",
            "Epoch: 12, Loss: 0.6139, Train: 85.91%, Valid: 85.14% Test: 85.88%\n",
            "Epoch: 13, Loss: 0.5939, Train: 86.17%, Valid: 85.46% Test: 85.94%\n",
            "Epoch: 14, Loss: 0.5842, Train: 86.27%, Valid: 85.54% Test: 86.04%\n",
            "Epoch: 15, Loss: 0.5730, Train: 86.48%, Valid: 85.86% Test: 86.22%\n",
            "Epoch: 16, Loss: 0.5613, Train: 86.71%, Valid: 85.98% Test: 86.29%\n",
            "Epoch: 17, Loss: 0.5540, Train: 86.77%, Valid: 86.00% Test: 86.40%\n",
            "Epoch: 18, Loss: 0.5471, Train: 86.83%, Valid: 86.06% Test: 86.42%\n",
            "Epoch: 19, Loss: 0.5418, Train: 86.84%, Valid: 86.06% Test: 86.39%\n",
            "Epoch: 20, Loss: 0.5333, Train: 86.79%, Valid: 85.98% Test: 86.35%\n",
            "Epoch: 21, Loss: 0.5253, Train: 86.92%, Valid: 86.02% Test: 86.38%\n",
            "Epoch: 22, Loss: 0.5182, Train: 87.09%, Valid: 86.28% Test: 86.53%\n",
            "Epoch: 23, Loss: 0.5121, Train: 87.41%, Valid: 86.62% Test: 86.72%\n",
            "Epoch: 24, Loss: 0.5063, Train: 87.53%, Valid: 86.78% Test: 86.88%\n",
            "Epoch: 25, Loss: 0.5021, Train: 87.65%, Valid: 87.02% Test: 86.94%\n",
            "Epoch: 26, Loss: 0.4943, Train: 87.75%, Valid: 87.16% Test: 87.06%\n",
            "Epoch: 27, Loss: 0.4909, Train: 87.79%, Valid: 87.08% Test: 87.09%\n",
            "Epoch: 28, Loss: 0.4878, Train: 87.86%, Valid: 87.08% Test: 87.19%\n",
            "Epoch: 29, Loss: 0.4813, Train: 87.97%, Valid: 87.24% Test: 87.29%\n",
            "Epoch: 30, Loss: 0.4800, Train: 88.09%, Valid: 87.42% Test: 87.43%\n",
            "Epoch: 31, Loss: 0.4747, Train: 88.16%, Valid: 87.54% Test: 87.56%\n",
            "Epoch: 32, Loss: 0.4703, Train: 88.20%, Valid: 87.64% Test: 87.63%\n",
            "Epoch: 33, Loss: 0.4677, Train: 88.24%, Valid: 87.62% Test: 87.69%\n",
            "Epoch: 34, Loss: 0.4668, Train: 88.31%, Valid: 87.72% Test: 87.75%\n",
            "Epoch: 35, Loss: 0.4612, Train: 88.44%, Valid: 87.90% Test: 87.86%\n",
            "Epoch: 36, Loss: 0.4567, Train: 88.53%, Valid: 88.14% Test: 88.00%\n",
            "Epoch: 37, Loss: 0.4532, Train: 88.58%, Valid: 88.18% Test: 88.01%\n",
            "Epoch: 38, Loss: 0.4502, Train: 88.64%, Valid: 88.14% Test: 88.13%\n",
            "Epoch: 39, Loss: 0.4476, Train: 88.69%, Valid: 88.26% Test: 88.24%\n",
            "Epoch: 40, Loss: 0.4451, Train: 88.70%, Valid: 88.20% Test: 88.10%\n",
            "Epoch: 41, Loss: 0.4426, Train: 88.71%, Valid: 87.96% Test: 88.16%\n",
            "Epoch: 42, Loss: 0.4400, Train: 88.72%, Valid: 87.92% Test: 88.20%\n",
            "Epoch: 43, Loss: 0.4401, Train: 88.74%, Valid: 88.06% Test: 88.26%\n",
            "Epoch: 44, Loss: 0.4332, Train: 88.75%, Valid: 88.20% Test: 88.25%\n",
            "Epoch: 45, Loss: 0.4316, Train: 88.82%, Valid: 88.20% Test: 88.33%\n",
            "Epoch: 46, Loss: 0.4301, Train: 88.88%, Valid: 88.26% Test: 88.33%\n",
            "Epoch: 47, Loss: 0.4273, Train: 88.94%, Valid: 88.26% Test: 88.37%\n",
            "Epoch: 48, Loss: 0.4250, Train: 88.98%, Valid: 88.30% Test: 88.43%\n",
            "Epoch: 49, Loss: 0.4226, Train: 88.97%, Valid: 88.20% Test: 88.48%\n",
            "Epoch: 50, Loss: 0.4211, Train: 88.96%, Valid: 88.28% Test: 88.44%\n",
            "Epoch: 51, Loss: 0.4186, Train: 89.13%, Valid: 88.52% Test: 88.59%\n",
            "Epoch: 52, Loss: 0.4186, Train: 89.26%, Valid: 88.58% Test: 88.69%\n",
            "Epoch: 53, Loss: 0.4149, Train: 89.19%, Valid: 88.56% Test: 88.61%\n",
            "Epoch: 54, Loss: 0.4117, Train: 89.05%, Valid: 88.48% Test: 88.53%\n",
            "Epoch: 55, Loss: 0.4104, Train: 89.24%, Valid: 88.50% Test: 88.64%\n",
            "Epoch: 56, Loss: 0.4109, Train: 89.41%, Valid: 88.68% Test: 88.77%\n",
            "Epoch: 57, Loss: 0.4088, Train: 89.46%, Valid: 88.70% Test: 88.85%\n",
            "Epoch: 58, Loss: 0.4062, Train: 89.47%, Valid: 88.72% Test: 88.85%\n",
            "Epoch: 59, Loss: 0.4054, Train: 89.44%, Valid: 88.72% Test: 88.93%\n",
            "Epoch: 60, Loss: 0.4027, Train: 89.55%, Valid: 88.68% Test: 88.90%\n",
            "Epoch: 61, Loss: 0.4022, Train: 89.56%, Valid: 88.78% Test: 88.93%\n",
            "Epoch: 62, Loss: 0.3988, Train: 89.57%, Valid: 88.78% Test: 88.86%\n",
            "Epoch: 63, Loss: 0.3972, Train: 89.55%, Valid: 88.76% Test: 88.81%\n",
            "Epoch: 64, Loss: 0.3960, Train: 89.61%, Valid: 88.94% Test: 88.92%\n",
            "Epoch: 65, Loss: 0.3961, Train: 89.70%, Valid: 89.00% Test: 89.11%\n",
            "Epoch: 66, Loss: 0.3921, Train: 89.82%, Valid: 88.94% Test: 89.21%\n",
            "Epoch: 67, Loss: 0.3925, Train: 89.78%, Valid: 89.12% Test: 89.17%\n",
            "Epoch: 68, Loss: 0.3906, Train: 89.83%, Valid: 89.14% Test: 89.10%\n",
            "Epoch: 69, Loss: 0.3889, Train: 89.83%, Valid: 89.16% Test: 89.11%\n",
            "Epoch: 70, Loss: 0.3885, Train: 89.83%, Valid: 89.20% Test: 89.18%\n",
            "Epoch: 71, Loss: 0.3854, Train: 89.83%, Valid: 89.08% Test: 89.10%\n",
            "Epoch: 72, Loss: 0.3844, Train: 89.87%, Valid: 89.06% Test: 89.15%\n",
            "Epoch: 73, Loss: 0.3834, Train: 89.94%, Valid: 89.12% Test: 89.17%\n",
            "Epoch: 74, Loss: 0.3845, Train: 90.01%, Valid: 89.16% Test: 89.20%\n",
            "Epoch: 75, Loss: 0.3805, Train: 90.03%, Valid: 89.06% Test: 89.25%\n",
            "Epoch: 76, Loss: 0.3819, Train: 90.08%, Valid: 89.18% Test: 89.32%\n",
            "Epoch: 77, Loss: 0.3789, Train: 90.06%, Valid: 89.10% Test: 89.39%\n",
            "Epoch: 78, Loss: 0.3764, Train: 90.06%, Valid: 89.16% Test: 89.36%\n",
            "Epoch: 79, Loss: 0.3765, Train: 90.13%, Valid: 89.32% Test: 89.39%\n",
            "Epoch: 80, Loss: 0.3770, Train: 90.06%, Valid: 89.28% Test: 89.33%\n",
            "Epoch: 81, Loss: 0.3776, Train: 90.10%, Valid: 89.30% Test: 89.41%\n",
            "Epoch: 82, Loss: 0.3724, Train: 90.15%, Valid: 89.32% Test: 89.41%\n",
            "Epoch: 83, Loss: 0.3740, Train: 90.17%, Valid: 89.18% Test: 89.31%\n",
            "Epoch: 84, Loss: 0.3720, Train: 90.20%, Valid: 89.52% Test: 89.37%\n",
            "Epoch: 85, Loss: 0.3703, Train: 90.20%, Valid: 89.34% Test: 89.41%\n",
            "Epoch: 86, Loss: 0.3697, Train: 90.21%, Valid: 89.44% Test: 89.32%\n",
            "Epoch: 87, Loss: 0.3679, Train: 90.25%, Valid: 89.44% Test: 89.38%\n",
            "Epoch: 88, Loss: 0.3677, Train: 90.28%, Valid: 89.52% Test: 89.49%\n",
            "Epoch: 89, Loss: 0.3662, Train: 90.40%, Valid: 89.46% Test: 89.57%\n",
            "Epoch: 90, Loss: 0.3645, Train: 90.38%, Valid: 89.36% Test: 89.44%\n",
            "Epoch: 91, Loss: 0.3647, Train: 90.38%, Valid: 89.46% Test: 89.53%\n",
            "Epoch: 92, Loss: 0.3647, Train: 90.41%, Valid: 89.52% Test: 89.55%\n",
            "Epoch: 93, Loss: 0.3633, Train: 90.42%, Valid: 89.52% Test: 89.55%\n",
            "Epoch: 94, Loss: 0.3634, Train: 90.43%, Valid: 89.54% Test: 89.65%\n",
            "Epoch: 95, Loss: 0.3617, Train: 90.44%, Valid: 89.82% Test: 89.74%\n",
            "Epoch: 96, Loss: 0.3607, Train: 90.38%, Valid: 89.42% Test: 89.59%\n",
            "Epoch: 97, Loss: 0.3631, Train: 90.46%, Valid: 89.48% Test: 89.60%\n",
            "Epoch: 98, Loss: 0.3595, Train: 90.42%, Valid: 89.68% Test: 89.67%\n",
            "Epoch: 99, Loss: 0.3617, Train: 90.52%, Valid: 89.78% Test: 89.61%\n",
            "Epoch: 100, Loss: 0.3584, Train: 90.40%, Valid: 89.50% Test: 89.43%\n",
            "Epoch: 101, Loss: 0.3586, Train: 90.59%, Valid: 89.84% Test: 89.72%\n",
            "Epoch: 102, Loss: 0.3553, Train: 90.56%, Valid: 89.92% Test: 89.69%\n",
            "Epoch: 103, Loss: 0.3552, Train: 90.41%, Valid: 89.62% Test: 89.50%\n",
            "Epoch: 104, Loss: 0.3561, Train: 90.64%, Valid: 89.68% Test: 89.66%\n",
            "Epoch: 105, Loss: 0.3527, Train: 90.64%, Valid: 89.84% Test: 89.73%\n",
            "Epoch: 106, Loss: 0.3540, Train: 90.54%, Valid: 89.58% Test: 89.50%\n",
            "Epoch: 107, Loss: 0.3524, Train: 90.46%, Valid: 89.40% Test: 89.46%\n",
            "Epoch: 108, Loss: 0.3518, Train: 90.48%, Valid: 89.66% Test: 89.64%\n",
            "Epoch: 109, Loss: 0.3535, Train: 90.57%, Valid: 89.66% Test: 89.63%\n",
            "Epoch: 110, Loss: 0.3507, Train: 90.58%, Valid: 89.70% Test: 89.71%\n",
            "Epoch: 111, Loss: 0.3489, Train: 90.57%, Valid: 89.74% Test: 89.79%\n",
            "Epoch: 112, Loss: 0.3481, Train: 90.67%, Valid: 89.94% Test: 89.74%\n",
            "Epoch: 113, Loss: 0.3476, Train: 90.58%, Valid: 89.66% Test: 89.61%\n",
            "Epoch: 114, Loss: 0.3478, Train: 90.74%, Valid: 90.18% Test: 89.83%\n",
            "Epoch: 115, Loss: 0.3464, Train: 90.65%, Valid: 89.84% Test: 89.80%\n",
            "Epoch: 116, Loss: 0.3469, Train: 90.77%, Valid: 90.00% Test: 89.75%\n",
            "Epoch: 117, Loss: 0.3450, Train: 90.79%, Valid: 90.04% Test: 89.76%\n",
            "Epoch: 118, Loss: 0.3456, Train: 90.78%, Valid: 90.00% Test: 89.85%\n",
            "Epoch: 119, Loss: 0.3438, Train: 90.78%, Valid: 89.98% Test: 89.85%\n",
            "Epoch: 120, Loss: 0.3461, Train: 90.76%, Valid: 89.72% Test: 89.79%\n",
            "Epoch: 121, Loss: 0.3450, Train: 90.72%, Valid: 89.88% Test: 89.83%\n",
            "Epoch: 122, Loss: 0.3452, Train: 90.83%, Valid: 89.88% Test: 89.90%\n",
            "Epoch: 123, Loss: 0.3408, Train: 90.75%, Valid: 89.98% Test: 89.74%\n",
            "Epoch: 124, Loss: 0.3424, Train: 90.84%, Valid: 90.20% Test: 89.89%\n",
            "Epoch: 125, Loss: 0.3426, Train: 90.93%, Valid: 90.28% Test: 90.01%\n",
            "Epoch: 126, Loss: 0.3422, Train: 90.84%, Valid: 89.82% Test: 89.97%\n",
            "Epoch: 127, Loss: 0.3403, Train: 90.81%, Valid: 89.78% Test: 89.87%\n",
            "Epoch: 128, Loss: 0.3375, Train: 90.79%, Valid: 89.88% Test: 89.88%\n",
            "Epoch: 129, Loss: 0.3372, Train: 90.93%, Valid: 90.10% Test: 90.00%\n",
            "Epoch: 130, Loss: 0.3376, Train: 90.92%, Valid: 89.90% Test: 89.89%\n",
            "Epoch: 131, Loss: 0.3368, Train: 91.00%, Valid: 90.20% Test: 89.99%\n",
            "Epoch: 132, Loss: 0.3373, Train: 90.96%, Valid: 90.28% Test: 89.91%\n",
            "Epoch: 133, Loss: 0.3354, Train: 90.92%, Valid: 90.08% Test: 89.91%\n",
            "Epoch: 134, Loss: 0.3341, Train: 91.05%, Valid: 90.28% Test: 89.87%\n",
            "Epoch: 135, Loss: 0.3343, Train: 91.05%, Valid: 90.32% Test: 90.18%\n",
            "Epoch: 136, Loss: 0.3340, Train: 90.95%, Valid: 90.18% Test: 90.15%\n",
            "Epoch: 137, Loss: 0.3348, Train: 91.10%, Valid: 90.10% Test: 90.03%\n",
            "Epoch: 138, Loss: 0.3337, Train: 91.07%, Valid: 90.10% Test: 90.01%\n",
            "Epoch: 139, Loss: 0.3321, Train: 91.07%, Valid: 90.16% Test: 90.03%\n",
            "Epoch: 140, Loss: 0.3307, Train: 90.86%, Valid: 89.74% Test: 89.82%\n",
            "Epoch: 141, Loss: 0.3330, Train: 91.22%, Valid: 90.32% Test: 90.12%\n",
            "Epoch: 142, Loss: 0.3318, Train: 91.18%, Valid: 90.42% Test: 90.03%\n",
            "Epoch: 143, Loss: 0.3312, Train: 91.05%, Valid: 89.84% Test: 89.90%\n",
            "Epoch: 144, Loss: 0.3285, Train: 91.05%, Valid: 89.96% Test: 89.95%\n",
            "Epoch: 145, Loss: 0.3293, Train: 91.12%, Valid: 90.28% Test: 90.00%\n",
            "Epoch: 146, Loss: 0.3279, Train: 91.12%, Valid: 90.38% Test: 90.07%\n",
            "Epoch: 147, Loss: 0.3284, Train: 91.20%, Valid: 90.34% Test: 90.11%\n",
            "Epoch: 148, Loss: 0.3267, Train: 91.25%, Valid: 90.48% Test: 90.24%\n",
            "Epoch: 149, Loss: 0.3266, Train: 91.24%, Valid: 90.22% Test: 90.18%\n",
            "Epoch: 150, Loss: 0.3243, Train: 91.26%, Valid: 90.32% Test: 90.17%\n",
            "Epoch: 151, Loss: 0.3253, Train: 91.27%, Valid: 90.26% Test: 90.11%\n",
            "Epoch: 152, Loss: 0.3253, Train: 91.32%, Valid: 90.26% Test: 90.25%\n",
            "Epoch: 153, Loss: 0.3229, Train: 91.31%, Valid: 90.30% Test: 90.17%\n",
            "Epoch: 154, Loss: 0.3233, Train: 91.31%, Valid: 90.46% Test: 90.15%\n",
            "Epoch: 155, Loss: 0.3259, Train: 91.14%, Valid: 90.18% Test: 90.11%\n",
            "Epoch: 156, Loss: 0.3231, Train: 91.03%, Valid: 90.24% Test: 90.05%\n",
            "Epoch: 157, Loss: 0.3247, Train: 91.22%, Valid: 90.36% Test: 90.06%\n",
            "Epoch: 158, Loss: 0.3231, Train: 91.31%, Valid: 90.08% Test: 90.16%\n",
            "Epoch: 159, Loss: 0.3225, Train: 91.27%, Valid: 90.06% Test: 90.20%\n",
            "Epoch: 160, Loss: 0.3221, Train: 91.34%, Valid: 90.42% Test: 90.26%\n",
            "Epoch: 161, Loss: 0.3199, Train: 91.33%, Valid: 90.52% Test: 90.27%\n",
            "Epoch: 162, Loss: 0.3216, Train: 91.34%, Valid: 90.56% Test: 90.09%\n",
            "Epoch: 163, Loss: 0.3208, Train: 91.35%, Valid: 90.38% Test: 90.04%\n",
            "Epoch: 164, Loss: 0.3215, Train: 91.27%, Valid: 90.32% Test: 90.20%\n",
            "Epoch: 165, Loss: 0.3216, Train: 91.19%, Valid: 90.16% Test: 89.98%\n",
            "Epoch: 166, Loss: 0.3226, Train: 91.34%, Valid: 90.52% Test: 90.15%\n",
            "Epoch: 167, Loss: 0.3200, Train: 91.35%, Valid: 90.46% Test: 90.17%\n",
            "Epoch: 168, Loss: 0.3184, Train: 91.39%, Valid: 90.32% Test: 90.03%\n",
            "Epoch: 169, Loss: 0.3181, Train: 91.50%, Valid: 90.50% Test: 90.35%\n",
            "Epoch: 170, Loss: 0.3173, Train: 91.41%, Valid: 90.28% Test: 90.36%\n",
            "Epoch: 171, Loss: 0.3162, Train: 91.24%, Valid: 89.90% Test: 90.07%\n",
            "Epoch: 172, Loss: 0.3191, Train: 91.49%, Valid: 90.42% Test: 90.39%\n",
            "Epoch: 173, Loss: 0.3173, Train: 91.45%, Valid: 90.36% Test: 90.34%\n",
            "Epoch: 174, Loss: 0.3159, Train: 91.45%, Valid: 90.52% Test: 90.38%\n",
            "Epoch: 175, Loss: 0.3168, Train: 91.53%, Valid: 90.56% Test: 90.37%\n",
            "Epoch: 176, Loss: 0.3145, Train: 91.44%, Valid: 90.54% Test: 90.25%\n",
            "Epoch: 177, Loss: 0.3150, Train: 91.48%, Valid: 90.42% Test: 90.33%\n",
            "Epoch: 178, Loss: 0.3150, Train: 91.46%, Valid: 90.20% Test: 90.37%\n",
            "Epoch: 179, Loss: 0.3134, Train: 91.57%, Valid: 90.36% Test: 90.47%\n",
            "Epoch: 180, Loss: 0.3134, Train: 91.55%, Valid: 90.38% Test: 90.34%\n",
            "Epoch: 181, Loss: 0.3122, Train: 91.59%, Valid: 90.44% Test: 90.31%\n",
            "Epoch: 182, Loss: 0.3124, Train: 91.54%, Valid: 90.46% Test: 90.29%\n",
            "Epoch: 183, Loss: 0.3140, Train: 91.33%, Valid: 90.12% Test: 90.05%\n",
            "Epoch: 184, Loss: 0.3126, Train: 91.56%, Valid: 90.48% Test: 90.42%\n",
            "Epoch: 185, Loss: 0.3117, Train: 91.53%, Valid: 90.40% Test: 90.35%\n",
            "Epoch: 186, Loss: 0.3112, Train: 91.38%, Valid: 90.36% Test: 90.22%\n",
            "Epoch: 187, Loss: 0.3137, Train: 91.58%, Valid: 90.66% Test: 90.47%\n",
            "Epoch: 188, Loss: 0.3119, Train: 91.47%, Valid: 90.46% Test: 90.31%\n",
            "Epoch: 189, Loss: 0.3159, Train: 91.39%, Valid: 90.20% Test: 90.12%\n",
            "Epoch: 190, Loss: 0.3125, Train: 91.59%, Valid: 90.36% Test: 90.23%\n",
            "Epoch: 191, Loss: 0.3107, Train: 91.46%, Valid: 90.26% Test: 90.23%\n",
            "Epoch: 192, Loss: 0.3101, Train: 91.59%, Valid: 90.48% Test: 90.33%\n",
            "Epoch: 193, Loss: 0.3081, Train: 91.73%, Valid: 90.32% Test: 90.53%\n",
            "Epoch: 194, Loss: 0.3095, Train: 91.43%, Valid: 89.88% Test: 90.08%\n",
            "Epoch: 195, Loss: 0.3106, Train: 91.64%, Valid: 90.28% Test: 90.58%\n",
            "Epoch: 196, Loss: 0.3111, Train: 91.62%, Valid: 90.58% Test: 90.46%\n",
            "Epoch: 197, Loss: 0.3075, Train: 91.64%, Valid: 90.50% Test: 90.44%\n",
            "Epoch: 198, Loss: 0.3049, Train: 91.65%, Valid: 90.58% Test: 90.29%\n",
            "Epoch: 199, Loss: 0.3092, Train: 91.53%, Valid: 90.48% Test: 90.18%\n",
            "Epoch: 200, Loss: 0.3060, Train: 91.72%, Valid: 90.72% Test: 90.38%\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(data.x, data.adj_t)\n",
        "  loss = F.nll_loss(out[train_idx], data.y[train_idx].reshape(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  result = test(model, data, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss.item():.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
